# æ¨¡å‹æä¾›è€…é…ç½®æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

ç³»ç»Ÿå·²å®ç°æ¨¡å‹æä¾›è€…æŠ½è±¡å±‚ï¼Œæ”¯æŒé€šè¿‡é…ç½®æ–‡ä»¶é€‰æ‹©ä½¿ç”¨ **APIäº‘ä¸Šæ¨¡å‹** æˆ– **æœ¬åœ°éƒ¨ç½²æ¨¡å‹**ï¼Œå®ç°ä»£ç ä¸æ¨¡å‹çš„è§£è€¦ã€‚

## ğŸ—ï¸ æ¶æ„è®¾è®¡

```
LLMClient (ç»Ÿä¸€æ¥å£)
    â”‚
    â”œâ”€ ModelProviderFactory (å·¥å‚)
    â”‚   â”‚
    â”‚   â”œâ”€ APIModelProvider (APIäº‘ä¸Šæ¨¡å‹)
    â”‚   â”‚   â””â”€ é€šè¿‡HTTP APIè°ƒç”¨è¿œç¨‹æ¨¡å‹
    â”‚   â”‚
    â”‚   â””â”€ LocalModelProvider (æœ¬åœ°éƒ¨ç½²æ¨¡å‹)
    â”‚       â””â”€ ä½¿ç”¨transformersåŠ è½½æœ¬åœ°æ¨¡å‹
```

## ğŸ“ é…ç½®æ–‡ä»¶

### é…ç½®æ–‡ä»¶ä½ç½®

`config/config.yaml`

### é…ç½®ç¤ºä¾‹

#### 1. ä½¿ç”¨APIäº‘ä¸Šæ¨¡å‹ï¼ˆé»˜è®¤ï¼‰

```yaml
model:
  # æä¾›è€…ç±»å‹: "api" è¡¨ç¤ºä½¿ç”¨APIäº‘ä¸Šæ¨¡å‹
  provider: "api"
  
  # APIé…ç½®
  api_base: "https://newapi.3173721.xyz/v1/chat/completions"
  api_key: "sk-DwBE5H6xxCV6I7i0q8v6rq3ZHauPuSq6fWVerxu7gJ9DmQoz"
  
  # é€šç”¨é…ç½®
  model_name: "qwen3-max"
  temperature: 0.1
  max_tokens: 2000
  timeout: 60
```

#### 2. ä½¿ç”¨æœ¬åœ°éƒ¨ç½²æ¨¡å‹

```yaml
model:
  # æä¾›è€…ç±»å‹: "local" è¡¨ç¤ºä½¿ç”¨æœ¬åœ°éƒ¨ç½²æ¨¡å‹
  provider: "local"
  
  # æœ¬åœ°æ¨¡å‹é…ç½®
  model_path: "/path/to/local/model"  # æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆHuggingFaceæ ¼å¼ï¼‰
  device: "cuda"  # è®¾å¤‡: "cuda" æˆ– "cpu"
  load_in_8bit: false  # æ˜¯å¦ä½¿ç”¨8bité‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
  load_in_4bit: false  # æ˜¯å¦ä½¿ç”¨4bité‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
  
  # é€šç”¨é…ç½®
  model_name: "qwen-7b-chat"  # æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
  temperature: 0.1
  max_tokens: 2000
  timeout: 60
```

## ğŸ”§ é…ç½®è¯´æ˜

### provider å­—æ®µ

| å€¼ | è¯´æ˜ | ä½¿ç”¨åœºæ™¯ |
|---|------|---------|
| `"api"` | APIäº‘ä¸Šæ¨¡å‹ï¼ˆé»˜è®¤ï¼‰ | ä½¿ç”¨è¿œç¨‹APIæœåŠ¡ |
| `"openai"` | OpenAIå…¼å®¹API | ä½¿ç”¨OpenAIæˆ–å…¼å®¹æœåŠ¡ |
| `"custom"` | è‡ªå®šä¹‰API | ä½¿ç”¨è‡ªå®šä¹‰APIæœåŠ¡ |
| `"cloud"` | äº‘æœåŠ¡ | ä½¿ç”¨äº‘æœåŠ¡æä¾›å•† |
| `"local"` | æœ¬åœ°éƒ¨ç½²æ¨¡å‹ | ä½¿ç”¨æœ¬åœ°HuggingFaceæ¨¡å‹ |
| `"huggingface"` | HuggingFaceæ¨¡å‹ | åŒlocal |
| `"transformers"` | Transformersæ¨¡å‹ | åŒlocal |

### APIäº‘ä¸Šæ¨¡å‹é…ç½®é¡¹

| é…ç½®é¡¹ | è¯´æ˜ | å¿…éœ€ | é»˜è®¤å€¼ |
|--------|------|------|--------|
| `api_base` | APIåŸºç¡€URL | æ˜¯ | - |
| `api_key` | APIå¯†é’¥ | æ˜¯ | - |
| `model_name` | æ¨¡å‹ID/åç§° | æ˜¯ | "qwen3-max" |
| `temperature` | æ¸©åº¦å‚æ•° | å¦ | 0.1 |
| `max_tokens` | æœ€å¤§tokenæ•° | å¦ | 2000 |
| `timeout` | è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰ | å¦ | 60 |

### æœ¬åœ°éƒ¨ç½²æ¨¡å‹é…ç½®é¡¹

| é…ç½®é¡¹ | è¯´æ˜ | å¿…éœ€ | é»˜è®¤å€¼ |
|--------|------|------|--------|
| `model_path` | æœ¬åœ°æ¨¡å‹è·¯å¾„ | æ˜¯ | - |
| `device` | è®¾å¤‡ï¼ˆcuda/cpuï¼‰ | å¦ | "cuda" |
| `load_in_8bit` | 8bité‡åŒ– | å¦ | false |
| `load_in_4bit` | 4bité‡åŒ– | å¦ | false |
| `model_name` | æ¨¡å‹åç§°ï¼ˆæ—¥å¿—ç”¨ï¼‰ | å¦ | - |
| `temperature` | æ¸©åº¦å‚æ•° | å¦ | 0.1 |
| `max_tokens` | æœ€å¤§tokenæ•° | å¦ | 2000 |

## ğŸŒ ç¯å¢ƒå˜é‡é…ç½®

ä¹Ÿå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ï¼ˆä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶ï¼‰ï¼š

```bash
# é€‰æ‹©æä¾›è€…ç±»å‹
export LLM_PROVIDER="api"  # æˆ– "local"

# APIäº‘ä¸Šæ¨¡å‹é…ç½®
export LLM_API_BASE="https://api.example.com/v1/chat/completions"
export LLM_API_KEY="your-api-key"
export LLM_MODEL="qwen3-max"

# æœ¬åœ°æ¨¡å‹é…ç½®
export LLM_MODEL_PATH="/path/to/local/model"
```

## ğŸ’» ä»£ç ä½¿ç”¨ç¤ºä¾‹

### ä½¿ç”¨APIäº‘ä¸Šæ¨¡å‹

```python
from src.llm import LLMClient

# æ–¹å¼1: ä½¿ç”¨é…ç½®æ–‡ä»¶
client = LLMClient()

# æ–¹å¼2: ç›´æ¥æŒ‡å®šé…ç½®
client = LLMClient(
    provider="api",
    api_base="https://api.example.com/v1/chat/completions",
    api_key="your-key",
    model="qwen3-max"
)

# ä½¿ç”¨
result = client.generate("ä½ å¥½")
```

### ä½¿ç”¨æœ¬åœ°éƒ¨ç½²æ¨¡å‹

```python
from src.llm import LLMClient

# æ–¹å¼1: ä½¿ç”¨é…ç½®æ–‡ä»¶
client = LLMClient()

# æ–¹å¼2: ç›´æ¥æŒ‡å®šé…ç½®
client = LLMClient(
    provider="local",
    config={
        "model_path": "/path/to/model",
        "device": "cuda",
        "model_name": "qwen-7b-chat"
    }
)

# ä½¿ç”¨ï¼ˆæ¥å£å®Œå…¨ä¸€è‡´ï¼‰
result = client.generate("ä½ å¥½")
```

## ğŸ”„ åˆ‡æ¢æ¨¡å‹æä¾›è€…

### æ–¹æ³•1: ä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `config/config.yaml`ï¼Œä¿®æ”¹ `provider` å­—æ®µï¼š

```yaml
model:
  provider: "local"  # ä» "api" æ”¹ä¸º "local"
  model_path: "/path/to/model"
```

### æ–¹æ³•2: ä½¿ç”¨ç¯å¢ƒå˜é‡

```bash
export LLM_PROVIDER="local"
export LLM_MODEL_PATH="/path/to/model"
```

### æ–¹æ³•3: ä»£ç ä¸­æŒ‡å®š

```python
client = LLMClient(provider="local", config={"model_path": "/path/to/model"})
```

## ğŸ“¦ ä¾èµ–è¦æ±‚

### APIäº‘ä¸Šæ¨¡å‹

- `requests` - HTTPè¯·æ±‚åº“

```bash
pip install requests
```

### æœ¬åœ°éƒ¨ç½²æ¨¡å‹

- `transformers` - HuggingFace Transformersåº“
- `torch` - PyTorchï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰

```bash
pip install transformers torch
```

å¯é€‰ï¼ˆé‡åŒ–æ”¯æŒï¼‰ï¼š
```bash
pip install bitsandbytes  # 8bit/4bité‡åŒ–
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### æœ¬åœ°æ¨¡å‹åŠ è½½

1. **å»¶è¿ŸåŠ è½½**: æœ¬åœ°æ¨¡å‹åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ‰åŠ è½½ï¼Œé¿å…å¯åŠ¨æ—¶å¡ä½
2. **æ˜¾å­˜è¦æ±‚**: ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPUæ˜¾å­˜ï¼ˆæˆ–ä½¿ç”¨CPUæ¨¡å¼ï¼‰
3. **æ¨¡å‹æ ¼å¼**: å¿…é¡»ä½¿ç”¨HuggingFaceæ ¼å¼çš„æ¨¡å‹

### APIæ¨¡å‹

1. **ç½‘ç»œè¿æ¥**: ç¡®ä¿å¯ä»¥è®¿é—®APIæœåŠ¡
2. **APIå¯†é’¥**: ç¡®ä¿APIå¯†é’¥æœ‰æ•ˆ
3. **è¶…æ—¶è®¾ç½®**: æ ¹æ®ç½‘ç»œæƒ…å†µè°ƒæ•´timeout

### é™çº§æœºåˆ¶

å¦‚æœæœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é™çº§åˆ°APIæä¾›è€…ï¼Œç¡®ä¿æœåŠ¡å¯ç”¨ã€‚

## ğŸ” è°ƒè¯•

### æŸ¥çœ‹å½“å‰ä½¿ç”¨çš„æä¾›è€…

```python
from src.llm import LLMClient

client = LLMClient()
print(f"Provider type: {type(client.provider).__name__}")
print(f"Model: {client.model}")
```

### æ—¥å¿—è¾“å‡º

ç³»ç»Ÿä¼šè®°å½•ä½¿ç”¨çš„æä¾›è€…ç±»å‹ï¼š

```
INFO: ä½¿ç”¨APIäº‘ä¸Šæ¨¡å‹æä¾›è€…
INFO: LLMClient initialized: provider=api, model=qwen3-max
```

æˆ–

```
INFO: ä½¿ç”¨æœ¬åœ°éƒ¨ç½²æ¨¡å‹æä¾›è€…
INFO: LocalModelProvider initialized: model_path=/path/to/model
```

## ğŸ“š å®Œæ•´é…ç½®ç¤ºä¾‹

### config.yaml (APIæ¨¡å¼)

```yaml
model:
  provider: "api"
  api_base: "https://newapi.3173721.xyz/v1/chat/completions"
  api_key: "sk-DwBE5H6xxCV6I7i0q8v6rq3ZHauPuSq6fWVerxu7gJ9DmQoz"
  model_name: "qwen3-max"
  temperature: 0.1
  max_tokens: 2000
  timeout: 60
```

### config.yaml (æœ¬åœ°æ¨¡å¼)

```yaml
model:
  provider: "local"
  model_path: "/home/user/models/qwen-7b-chat"
  device: "cuda"
  load_in_8bit: true  # å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ç”¨8bité‡åŒ–
  model_name: "qwen-7b-chat"
  temperature: 0.1
  max_tokens: 2000
  timeout: 60
```

## âœ… ä¼˜åŠ¿

1. **ä»£ç è§£è€¦**: ä¸šåŠ¡ä»£ç ä¸ä¾èµ–å…·ä½“çš„æ¨¡å‹å®ç°
2. **çµæ´»åˆ‡æ¢**: é€šè¿‡é…ç½®æ–‡ä»¶å³å¯åˆ‡æ¢æ¨¡å‹æä¾›è€…
3. **ç»Ÿä¸€æ¥å£**: æ— è®ºä½¿ç”¨å“ªç§æä¾›è€…ï¼Œè°ƒç”¨æ–¹å¼å®Œå…¨ä¸€è‡´
4. **æ˜“äºæ‰©å±•**: å¯ä»¥è½»æ¾æ·»åŠ æ–°çš„æ¨¡å‹æä¾›è€…ï¼ˆå¦‚vLLMã€TGIç­‰ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0  
**æœ€åæ›´æ–°**: 2026-01-28
